{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "respective-enough",
   "metadata": {},
   "source": [
    "# Laboratory 8.2 Artificial Intelligence Models - Carlos GarcÃ­a Calvo\n",
    "## Investigation of a practical application of Artificial Intelligence Yolo5v\n",
    "\n",
    "YOLO (You Only Look Once) is one of the most popular deep learning-based object detection algorithms.\n",
    "Yolov5 already allows us to create and train our own object detector. It is based on a neural network that divides the image into regions, predicting identification frames and probabilities for each region; The boxes are weighted based on the predicted probabilities. The algorithm learns generalizable representations of objects, allowing a low detection error for new inputs, different from the training data set.\n",
    "\n",
    "\n",
    "\n",
    "This program is made in a Jupyter notebook and programmed in python\n",
    "Requires installing the following libraries and programs from Anaconda:\n",
    "\n",
    "From Anaconda Terminal:\n",
    "\n",
    "- Step 1 - installing OpenCV\n",
    "pip install opencv-python\n",
    "python\n",
    "import cv2\n",
    "\n",
    "- Step 2 - Check\n",
    "OpenCV Version\n",
    "cv2.version\n",
    "exit()\n",
    "\n",
    "In case of failure, look at https://programarfacil.com/blog/vision-artificial/instalar-opencv-python-anaconda/\n",
    "\n",
    "- Step 3 - Installing YOLOv5 (requires Python>=3.7.0 and PyTorch>=1.7.\n",
    "\n",
    "pip install yolov5\n",
    "cd yolov5\n",
    "pip install -qr https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt\n",
    "sudo pip install scikit-learn\n",
    "\n",
    "Webography:\n",
    "\n",
    "ultralytics github by yolov5:\n",
    "https://docs.ultralytics.com/tutorials/pytorch-hub/\n",
    "https://github.com/ultralytics/yolov5\n",
    "\n",
    "About the openCV and Jupyter Notebooks windows problem: https://medium.com/@mrdatainsight/how-to-use-opencv-imshow-in-a-jupyter-notebook-quick-tip-ce83fa32b5ad\n",
    "\n",
    "Self-image training on Yolov5:\n",
    "\n",
    "https://morioh.com/p/f04ccdbc3381\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-atmosphere",
   "metadata": {},
   "source": [
    "### This code loads a pre-trained YOLOv5s model from PyTorch Hub as a model and passes an image to an already trained yolov5s database (it is the lightest and fastest image and data recognition model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-national",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# acquisition of the Yolo5v Model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # o yolov5n - yolov5x6..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-encyclopedia",
   "metadata": {},
   "source": [
    "### We prepare libraries for the acquisition of the images/treatment, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-completion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np              # maths\n",
    "import cv2\n",
    "from  matplotlib import pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heard-pound",
   "metadata": {},
   "source": [
    "### This code loads an image directly from the webcam and displays it in a separate window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-hollow",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "leido, frame = cap.read()\n",
    "\n",
    "if leido == True:\n",
    "\tcv2.imwrite(\"imagen.png\", frame)\n",
    "\tprint(\"Photo taken correctly and saved locally\")\n",
    "else:\n",
    "\tprint(\"Error accessing camera\")\n",
    "\n",
    "\"\"\"\n",
    "\tFinally we release or release the camera\n",
    "\"\"\"\n",
    "cap.release()\n",
    "\n",
    "\n",
    "im = cv2.imread('imagen.png',1)\n",
    "im2 = im[:,:,::-1] \n",
    "plt.imshow(im2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-bumper",
   "metadata": {},
   "source": [
    "Now we can load this image saved locally or we can also load an image from the internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-austria",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we load image from the internet\n",
    "\n",
    "img = 'imagen.png' # or file, address url, PIL, OpenCV, numpy, list...\n",
    "# imagen.png\n",
    "# https://ultralytics.com/images/zidane.jpg\n",
    "# https://www.monederosmart.com/wp-content/uploads/2020/10/Tjw-H0m-LsD-2B6-paraguas-2-40513352_s.jpg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-trace",
   "metadata": {},
   "source": [
    "### We perform model search on the already trained YOLO5 neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-variety",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "results = model(img)\n",
    "\n",
    "\n",
    "# Results\n",
    "results.print()  # or .show(), .save(), .crop(), .pandas(), etc.\n",
    "results.pandas().xyxy[0].sort_values('xmin')  # sorted left-right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-quarter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WE RECORD IMAGES AND SHOW THE RESULTS OBTAINED\n",
    "results.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-oxford",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 3):  # Update Number Based on Images Taken\n",
    "    path='runs\\detect\\exp'+str(i)+'\\imagen.jpg'\n",
    "    im = cv2.imread(path,1)\n",
    "    im2 = im[:,:,::-1] \n",
    "    plt.imshow(im2)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-monthly",
   "metadata": {},
   "source": [
    "### Now we're going to use YOLOv5 in a scene in Coppelia Sim so that the robot is able to classify objects by images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92900884",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sim          # library to connect with CoppeliaSim\n",
    "import sympy as sp  # library for symbolic calculation\n",
    "import torch\n",
    "import pandas\n",
    "import time\n",
    "\n",
    "import cv2 # opencv\n",
    "from matplotlib import pyplot as plt\n",
    "from sympy.physics.vector import init_vprinting\n",
    "\n",
    "import numpy as np\n",
    "from sympy import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcff8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect(port):\n",
    "# Establishes the connection to VREP\n",
    "# port must match the connection port in VREP\n",
    "# returns the client number or -1 if the connection cannot be established\n",
    "    sim.simxFinish(-1) # just in case, close all opened connections\n",
    "    clientID=sim.simxStart('127.0.0.1',port,True,True,2000,5) # Conectarse\n",
    "    if clientID == 0: print(\"connect to\", port)\n",
    "    else: print(\"Could not connect\")\n",
    "    return clientID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64afe82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We require the handlers for the joints, the suction cup and the suction sensor (Allows us to know if the object is close)\n",
    "clientID = connect(19999)\n",
    "\n",
    "retCode,tip=sim.simxGetObjectHandle(clientID,'suctionPadSensor',sim.simx_opmode_blocking)\n",
    "retCode,suction=sim.simxGetObjectHandle(clientID,'suctionPad',sim.simx_opmode_blocking)\n",
    "retCode,camera=sim.simxGetObjectHandle(clientID,'Vision_sensor',sim.simx_opmode_blocking)\n",
    "retCode,joint1=sim.simxGetObjectHandle(clientID,'Joint1',sim.simx_opmode_blocking)\n",
    "retCode,joint2=sim.simxGetObjectHandle(clientID,'Joint2',sim.simx_opmode_blocking)\n",
    "retCode,joint3=sim.simxGetObjectHandle(clientID,'Joint3',sim.simx_opmode_blocking)\n",
    "print(tip, suction, joint1, joint2, joint3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb944917",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We obtain the image from the Camera Sensor\n",
    "retCode, resolution, image = sim.simxGetVisionSensorImage(clientID,camera,0,sim.simx_opmode_oneshot_wait)\n",
    "img=np.array(image,dtype=np.uint8)\n",
    "img.resize([resolution[1],resolution[0],3])\n",
    "img = img[::-1, ::-1, :]\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c722db1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5x6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf3bade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "results = model(img)\n",
    "\n",
    "# Results\n",
    "positions = results.pandas().xyxy[0]\n",
    "positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5f39e9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# I calculate the centers of each detection\n",
    "positions[\"xcenter\"] = [(positions[\"xmax\"][i] - positions[\"xmin\"][i])/2 + positions[\"xmin\"][i] for i in range(len(positions))]\n",
    "positions[\"ycenter\"] = [(positions[\"ymax\"][i] - positions[\"ymin\"][i])/2 + positions[\"ymin\"][i] for i in range(len(positions))]\n",
    "positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7a52bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Change the centers from 0-1000 to 0-0.5 to use the map coordinates\n",
    "\n",
    "coords = np.array([[positions[\"xcenter\"][i]*0.5/1000, positions[\"ycenter\"][i]*0.5/1000] for i in range(len(positions))])\n",
    "positions[\"x\"] = coords[:, 0]\n",
    "positions[\"y\"] = coords[:, 1]\n",
    "positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638dc0d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Delete all necessary data\n",
    "\n",
    "detections = positions.drop(columns=[\"xmin\", \"ymin\", \"xmax\", \"ymax\", \"confidence\", \"class\", \"xcenter\", \"ycenter\"])\n",
    "detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec87a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the angels of the joints with inverse kinematics\n",
    "init_vprinting(use_latex='mathjax', pretty_print=False)\n",
    "\n",
    "from sympy.physics.mechanics import dynamicsymbols\n",
    "theta1, theta2, la, lb, theta, alpha, a, d = dynamicsymbols('theta1 theta2 la lb theta alpha a d')\n",
    "theta1, theta2, la, lb, theta, alpha, a, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6b65c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I calculate the matrix homogenies\n",
    "\n",
    "rot = sp.Matrix([[sp.cos(theta), -sp.sin(theta)*sp.cos(alpha), sp.sin(theta)*sp.sin(alpha)],\n",
    "                 [sp.sin(theta), sp.cos(theta)*sp.cos(alpha), -sp.cos(theta)*sp.sin(alpha)],\n",
    "                 [0, sp.sin(alpha), sp.cos(alpha)]])\n",
    "\n",
    "trans = sp.Matrix([a*sp.cos(theta),a*sp.sin(theta),d])\n",
    "\n",
    "last_row = sp.Matrix([[0, 0, 0, 1]])\n",
    "\n",
    "m = sp.Matrix.vstack(sp.Matrix.hstack(rot, trans), last_row)\n",
    "\n",
    "m01 = m.subs({ theta:theta1, d:0, a:la , alpha:0})\n",
    "m12 = m.subs({ theta:theta2, d:0, a:lb , alpha:0})\n",
    "\n",
    "m02 = (m01*m12)\n",
    "mbee= sp.Matrix([[sp.trigsimp(m02[0,0].simplify()), sp.trigsimp(m02[0,1].simplify()), sp.trigsimp(m02[0,2].simplify()),sp.trigsimp(m02[0,3].simplify())],\n",
    "                 [sp.trigsimp(m02[1,0].simplify()), sp.trigsimp(m02[1,1].simplify()), sp.trigsimp(m02[1,2].simplify()),sp.trigsimp(m02[1,3].simplify())],\n",
    "                 [m02[2,0].simplify(), m02[2,1].simplify(), m02[2,2].simplify(),m02[2,3].simplify()],\n",
    "                 [m02[3,0].simplify(), m02[3,1].simplify(), m02[3,2].simplify(),m02[3,3].simplify()]])\n",
    "\n",
    "mbee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef6dbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcula(p): # Calculate IK function\n",
    "    eq1 = 0.3 * cos(theta1) + 0.3 * cos(theta1 + theta2) - p[0]\n",
    "    eq2 = 0.3 * sin(theta1) + 0.3 * sin(theta1 + theta2) - p[1]\n",
    "    q = nsolve((eq1,eq2),(theta1,theta2),(1,1))\n",
    "    return q\n",
    "\n",
    "def mou(q): # Move joint0 & joint1  function\n",
    "    retCode = sim.simxSetJointTargetPosition(clientID, joint1, q[0], sim.simx_opmode_oneshot)\n",
    "    retCode = sim.simxSetJointTargetPosition(clientID, joint2, q[1], sim.simx_opmode_oneshot)\n",
    "    \n",
    "def setEffector(val):\n",
    "# function that triggers the end effector remotely\n",
    "# val is Int with value 0 or 1 to disable or activate the final actuator.\n",
    "    res,retInts,retFloats,retStrings,retBuffer=sim.simxCallScriptFunction(clientID,\n",
    "        \"suctionPad\", sim.sim_scripttype_childscript,\"setEffector\",[val],[],[],\"\", sim.simx_opmode_blocking)\n",
    "    return res\n",
    "\n",
    "def baixa(): # Down Manipulator function\n",
    "    retCode = sim.simxSetJointTargetPosition(clientID, joint3, -0.095, sim.simx_opmode_oneshot)\n",
    "     \n",
    "def puja(): # Up Manipulator function\n",
    "    retCode = sim.simxSetJointTargetPosition(clientID, joint3, 0, sim.simx_opmode_oneshot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-jewelry",
   "metadata": {},
   "source": [
    "### In this part you have to be able to stack the cats and dogs in different places (you can use the previous functions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4054ec57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-berry",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
